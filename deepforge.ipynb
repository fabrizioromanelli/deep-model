{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 18:21:57.482720: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-12 18:21:57.585528: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-12 18:21:59.270590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-12 18:21:59.274021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-12 18:21:59.274089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-12 18:21:59.884293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-12 18:21:59.884378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-12 18:21:59.884385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-12 18:21:59.884406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-12 18:21:59.884432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1585 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Jupyter notebook for testing deepforge library\n",
    "# author: Fabrizio Romanelli\n",
    "# email : fabrizio.romanelli@gmail.com\n",
    "# date  : 04/10/2023\n",
    "\n",
    "# Import the deepforge library\n",
    "import deepforge as df\n",
    "\n",
    "# Initialize the environment\n",
    "df.initialize(CPU=20, GPU=1, VERBOSE='2', NPARRAYS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Deep Neural Network example\n",
    "import numpy as np\n",
    "\n",
    "# Make an instance of a multivariate DNN\n",
    "mDnn = df.multivariateDNN(name=\"Simple DNN\", inputN=1)\n",
    "\n",
    "# Set inputs, inner layers and out layers\n",
    "mDnn.setInputs([{'shape': (2,), 'name': 'Input layer'}])\n",
    "mDnn.setLayers([[{'units': 16, 'activation': 'elu'}, {'units': 16, 'activation': 'elu'}, {'units': 16, 'activation': 'elu'}, {'units': 3, 'activation': 'linear'}]])\n",
    "mDnn.setOutLayers([{'units': 1, 'activation': 'linear'}])\n",
    "\n",
    "# Configure the model\n",
    "mDnn.setModelConfiguration(optimizer='adam', loss='mse')\n",
    "\n",
    "# Build the model and print the summary\n",
    "mDnn.build()\n",
    "mDnn.summary()\n",
    "\n",
    "# Train the model\n",
    "x1 = np.array([2,3,5,6,7], dtype=np.float32)\n",
    "x2 = np.array([1,2,4,5,6], dtype=np.float32)\n",
    "X1 = np.array([x1,x2], dtype=np.float32).T\n",
    "y  = np.array([3,4,6,7,8], dtype=np.float32)\n",
    "\n",
    "mDnn.fit(x=[X1], y=y, epochs=20, shuffle=True, verbose=0)\n",
    "\n",
    "# Save the model\n",
    "mDnn.save('simpleDNN',tflite=False)\n",
    "\n",
    "# Make a prediction with the model\n",
    "x1 = np.array([8], dtype=np.float32)\n",
    "x2 = np.array([7], dtype=np.float32)\n",
    "X1 = np.array([x1,x2], dtype=np.float32).T\n",
    "y = mDnn.predict([X1])\n",
    "print(y.numpy())\n",
    "\n",
    "# Load the model\n",
    "mDnnCopy = df.multivariateDNN(name=\"simple DNN 2\")\n",
    "mDnnCopy.load(\"simpleDNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network with 2 input layers and custom loss function example\n",
    "import numpy as np\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "# Define a custom loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "  mse = MeanSquaredError()\n",
    "  return mse(y_true, y_pred)\n",
    "\n",
    "# Make an instance of a multivariate DNN\n",
    "mDnn2 = df.multivariateDNN(name=\"multivariate DNN\", inputN=2)\n",
    "\n",
    "# Set inputs, inner layers and out layers\n",
    "mDnn2.setInputs([{'shape': (2,), 'name': 'inputLayer1'}, {'shape': (2,), 'name': 'inputLayer2'}])\n",
    "\n",
    "innerLayers = [[{'units': 32, 'activation': 'elu'}, {'units': 16, 'activation': 'elu'}, {'units': 8, 'activation': 'elu'}, {'units': 3, 'activation': 'linear'}]]\n",
    "innerLayers.append([{'units': 32, 'activation': 'elu'}, {'units': 16, 'activation': 'elu'}, {'units': 8, 'activation': 'elu'}, {'units': 3, 'activation': 'linear'}])\n",
    "mDnn2.setLayers(innerLayers)\n",
    "\n",
    "outputLayers = [{'units': 32, 'activation': 'elu'}, {'units': 1, 'activation': 'linear'}]\n",
    "mDnn2.setOutLayers(outputLayers)\n",
    "\n",
    "# Configure the model\n",
    "mDnn2.setModelConfiguration(optimizer='adam', loss=custom_loss)\n",
    "\n",
    "# Build the model and print the summary\n",
    "mDnn2.build()\n",
    "mDnn2.summary()\n",
    "\n",
    "# Train the model\n",
    "x1 = np.array([0.2,0.3,0.5,0.6,0.7], dtype=np.float32)\n",
    "x2 = np.array([1.1,1.2,1.4,1.5,1.6], dtype=np.float32)\n",
    "X1 = np.array([x1,x2], dtype=np.float32).T\n",
    "x3 = np.array([0.2,0.3,0.5,0.6,0.7], dtype=np.float32)\n",
    "x4 = np.array([1.1,1.2,1.4,1.5,1.6], dtype=np.float32)\n",
    "X2 = np.array([x3,x4], dtype=np.float32).T\n",
    "y  = np.array([3.0,4.0,6.0,7.0,8.0], dtype=np.float32)\n",
    "\n",
    "mDnn2.fit(x=[X1,X2], y=y, epochs=50, shuffle=True, verbose=0)\n",
    "\n",
    "# Save the model\n",
    "mDnn2.save('multivariateDNN',tflite=False)\n",
    "\n",
    "# Make a prediction with the model\n",
    "x1 = np.array([0.4], dtype=np.float32)\n",
    "x2 = np.array([1.3], dtype=np.float32)\n",
    "X1 = np.array([x1,x2], dtype=np.float32).T\n",
    "x3 = np.array([0.4], dtype=np.float32)\n",
    "x4 = np.array([1.3], dtype=np.float32)\n",
    "X2 = np.array([x3,x4], dtype=np.float32).T\n",
    "y = mDnn2.predict([X1,X2])\n",
    "print(y.numpy())\n",
    "\n",
    "# Load the model with the custom loss function\n",
    "mDnn2Copy = df.multivariateDNN(name=\"multivariate DNN 2\")\n",
    "mDnn2Copy.load(\"multivariateDNN\", custom_objects = {'custom_loss': custom_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network example with MNIST dataset training and validation\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Make an instance of a CNN\n",
    "cnn = df.CNN(name=\"Simple CNN\", inputN=1)\n",
    "\n",
    "# Set inputs, inner layers and out layers\n",
    "cnn.setInputs([{'shape': (28, 28, 1), 'name': 'Input Layer'}])\n",
    "cnn.setConvLayers([[{'filters': 32, 'kernel_size': (3, 3), 'activation': 'relu'},{'filters': 64, 'kernel_size': (3, 3), 'activation': 'relu'},{'filters': 64, 'kernel_size': (3, 3), 'activation': 'relu'}]])\n",
    "cnn.setPoolLayers([[{'pool_size': (2,2)},{'pool_size': (2,2)}]])\n",
    "cnn.setOutLayers([{'units': 64, 'activation': 'relu'},{'units': 10, 'activation': 'softmax'}])\n",
    "\n",
    "# Configure the model\n",
    "cnn.setModelConfiguration(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build the model and print the summary\n",
    "cnn.build()\n",
    "cnn.summary()\n",
    "\n",
    "# Load the MNIST dataset and preprocess it\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Fit the model\n",
    "cnn.fit(x=train_images, y=train_labels, epochs=5, batch_size=64)\n",
    "\n",
    "# Save the model\n",
    "cnn.save('CNN',tflite=False)\n",
    "\n",
    "# Get the Keras model and run a test to evaluate accuracy\n",
    "cnnModel = cnn.getModel()\n",
    "\n",
    "test_loss, test_acc = cnnModel.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network examples\n",
    "\n",
    "# Make an instance of a RNN\n",
    "rnn = df.RNN(name=\"Simple RNN\", inputN=1)\n",
    "\n",
    "# Set inputs, inner layers and out layers\n",
    "rnn.setInputs([{'shape': (1,2), 'name': 'Input layer'}])\n",
    "rnn.setRecurrentLayers([[{'units': 500}]])\n",
    "rnn.setOutLayers([{'units': 1, 'activation': 'linear'}])\n",
    "\n",
    "# Configure the model\n",
    "rnn.setModelConfiguration(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build the model and print the summary\n",
    "rnn.build()\n",
    "rnn.summary()\n",
    "\n",
    "################################################\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "# Another RNN example with 3 stacked LSTM layers\n",
    "rnn2 = df.RNN(name=\"Stacked RNN\")\n",
    "\n",
    "# Set inputs, inner layers and out layers\n",
    "rnn2.setInputs([{'shape': (6,2), 'name': 'Input layer'}])\n",
    "rnn2.setRecurrentLayers([[{'units': 500, 'return_sequences': True},{'units': 500, 'return_sequences': True},{'units': 500}]])\n",
    "rnn2.setOutLayers([{'units': 1, 'activation': 'linear'}])\n",
    "\n",
    "# Configure the model\n",
    "rnn2.setModelConfiguration(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build the model and print the summary\n",
    "rnn2.build()\n",
    "rnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Recurrent Neural Network example\n",
    "\n",
    "# Make an instance of a CRNN\n",
    "crnn = df.CRNN(name=\"Simple CRNN\", inputN=1)\n",
    "\n",
    "# Set inputs, inner layers and out layers\n",
    "crnn.setInputs([{'shape': (3,3,1), 'name': 'Input Layer'}])\n",
    "crnn.setConvLayers([[{'filters': 32, 'kernel_size': (2,2), 'activation': 'relu'},{'filters': 64, 'kernel_size': (2,2), 'activation': 'relu', 'padding': 'same'}]])\n",
    "crnn.setPoolLayers([[{'pool_size': (2,2)},{'pool_size': (2,2)}]])\n",
    "crnn.setRecurrentLayers([{'units': 500}])\n",
    "crnn.setOutLayers([{'units': 1, 'activation': 'linear'}])\n",
    "\n",
    "# Configure the model\n",
    "crnn.setModelConfiguration(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build the model and print the summary\n",
    "crnn.build()\n",
    "crnn.summary()\n",
    "\n",
    "# Save the model\n",
    "crnn.save('CRNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DF] Building model...\n",
      "[DF] Model built!\n",
      "Model: \"SimpleDAE\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input Layer (InputLayer)    [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 10)               40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_32 (LeakyReLU)  (None, 10)                0         \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 10)                60        \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 10)               40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_33 (LeakyReLU)  (None, 10)                0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 415\n",
      "Trainable params: 375\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 18:31:34.696256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.2500\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2487\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2474\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2462\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2449\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2436\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2423\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2410\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2397\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2384\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2371\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2357\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2344\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2331\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2317\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2304\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2290\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2277\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2263\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2249\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2236\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2222\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2208\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2194\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2180\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2166\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2152\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2137\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2123\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2109\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2094\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2080\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2065\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2051\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2036\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2021\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2006\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1991\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1976\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1961\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1946\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1931\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1916\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1901\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1885\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1870\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1854\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1839\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1823\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1808\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1792\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1776\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1760\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1745\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1729\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1713\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1697\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1681\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1665\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1649\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1633\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1617\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1601\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1584\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1568\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1552\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1536\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1520\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1503\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1487\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1471\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1454\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1438\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1422\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1406\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1389\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1373\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1357\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1341\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1324\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1308\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1292\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1276\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1260\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1243\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1227\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1211\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1195\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1179\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1163\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1148\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1132\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1116\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1100\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1085\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1069\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1053\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1038\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1023\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1007\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0992\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0977\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0962\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0947\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0932\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0917\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0902\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0888\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0873\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0858\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0844\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0830\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0816\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0802\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0788\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0774\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0760\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0747\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0733\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0720\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.0706\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0693\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0680\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0668\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0655\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0642\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0617\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0605\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0593\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0581\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0569\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0558\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0546\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0535\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0524\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0513\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0502\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0491\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0480\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0470\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0460\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0449\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0439\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0429\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0420\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0410\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0401\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0391\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0382\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0373\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0364\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0356\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0347\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0339\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0331\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0323\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0315\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0307\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0299\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0292\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0284\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0277\n",
      "Epoch 164/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0270\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0263\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0256\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0250\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0243\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0237\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0231\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0225\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0219\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0213\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0207\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0202\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0196\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0191\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0186\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0181\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0176\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0171\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0166\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0162\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0157\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0153\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0148\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0144\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0140\n",
      "Epoch 189/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0136\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0132\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0129\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0125\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0122\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0118\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0115\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0111\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0108\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0105\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0102\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0099\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0096\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0094\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0091\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0088\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0086\n",
      "Epoch 206/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0083\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0081\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0079\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0076\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0074\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0072\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0070\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0068\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0066\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0064\n",
      "Epoch 216/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0062\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0060\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0059\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0057\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0055\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0054\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0052\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0051\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0049\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0048\n",
      "Epoch 226/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0046\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0045\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0044\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0043\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0041\n",
      "Epoch 231/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0040\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0039\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0038\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0037\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0036\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0035\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0034\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0033\n",
      "Epoch 239/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0032\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0031\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0030\n",
      "Epoch 242/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0029\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0028\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0028\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0027\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0026\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0025\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0025\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0024\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0023\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0023\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0022\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0021\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0021\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0020\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0020\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0019\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0019\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0018\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0018\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0017\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0017\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0016\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0016\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0015\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0015\n",
      "Epoch 267/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0014\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0014\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0014\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0013\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0012\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0012\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0012\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0011\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0011\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0011\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0010\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0010\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.9113e-04\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.6322e-04\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.3609e-04\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.0971e-04\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 8.8405e-04\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.5911e-04\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.3486e-04\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.1127e-04\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.8834e-04\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.6605e-04\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.4437e-04\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2329e-04\n",
      "Epoch 292/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.0280e-04\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.8287e-04\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.6350e-04\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.4466e-04\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 6.2634e-04\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 6.0853e-04\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.9122e-04\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.7438e-04\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.5802e-04\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.4210e-04\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.2663e-04\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.1159e-04\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.9697e-04\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.8276e-04\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.6894e-04\n",
      "Epoch 307/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.5551e-04\n",
      "Epoch 308/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.4245e-04\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2975e-04\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.1741e-04\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.0542e-04\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.9376e-04\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8242e-04\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.7141e-04\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.6070e-04\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.5029e-04\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.4018e-04\n",
      "Epoch 318/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.3035e-04\n",
      "Epoch 319/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.2079e-04\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.1151e-04\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0248e-04\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9371e-04\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8519e-04\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.7691e-04\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.6886e-04\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.6104e-04\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.5344e-04\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.4606e-04\n",
      "Epoch 329/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3888e-04\n",
      "Epoch 330/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3191e-04\n",
      "Epoch 331/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2514e-04\n",
      "Epoch 332/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1856e-04\n",
      "Epoch 333/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1216e-04\n",
      "Epoch 334/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0595e-04\n",
      "Epoch 335/400\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.9992e-04\n",
      "Epoch 336/400\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.9405e-04\n",
      "Epoch 337/400\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8836e-04\n",
      "Epoch 338/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8283e-04\n",
      "Epoch 339/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.7745e-04\n",
      "Epoch 340/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7223e-04\n",
      "Epoch 341/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6716e-04\n",
      "Epoch 342/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.6223e-04\n",
      "Epoch 343/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5745e-04\n",
      "Epoch 344/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5280e-04\n",
      "Epoch 345/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.4828e-04\n",
      "Epoch 346/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4390e-04\n",
      "Epoch 347/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3964e-04\n",
      "Epoch 348/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3551e-04\n",
      "Epoch 349/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.3149e-04\n",
      "Epoch 350/400\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2759e-04\n",
      "Epoch 351/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2380e-04\n",
      "Epoch 352/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2012e-04\n",
      "Epoch 353/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1655e-04\n",
      "Epoch 354/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1308e-04\n",
      "Epoch 355/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0971e-04\n",
      "Epoch 356/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0644e-04\n",
      "Epoch 357/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0326e-04\n",
      "Epoch 358/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0018e-04\n",
      "Epoch 359/400\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.7184e-05\n",
      "Epoch 360/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.4277e-05\n",
      "Epoch 361/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.1454e-05\n",
      "Epoch 362/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.8713e-05\n",
      "Epoch 363/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.6053e-05\n",
      "Epoch 364/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.3470e-05\n",
      "Epoch 365/400\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 8.0962e-05\n",
      "Epoch 366/400\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.8527e-05\n",
      "Epoch 367/400\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.6164e-05\n",
      "Epoch 368/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.3869e-05\n",
      "Epoch 369/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.1642e-05\n",
      "Epoch 370/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.9480e-05\n",
      "Epoch 371/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.7382e-05\n",
      "Epoch 372/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.5345e-05\n",
      "Epoch 373/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.3368e-05\n",
      "Epoch 374/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.1449e-05\n",
      "Epoch 375/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.9587e-05\n",
      "Epoch 376/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.7779e-05\n",
      "Epoch 377/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.6025e-05\n",
      "Epoch 378/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.4322e-05\n",
      "Epoch 379/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.2670e-05\n",
      "Epoch 380/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 5.1066e-05\n",
      "Epoch 381/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.9510e-05\n",
      "Epoch 382/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.8000e-05\n",
      "Epoch 383/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.6534e-05\n",
      "Epoch 384/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.5113e-05\n",
      "Epoch 385/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3733e-05\n",
      "Epoch 386/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.2394e-05\n",
      "Epoch 387/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.1095e-05\n",
      "Epoch 388/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.9835e-05\n",
      "Epoch 389/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8612e-05\n",
      "Epoch 390/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.7425e-05\n",
      "Epoch 391/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.6274e-05\n",
      "Epoch 392/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.5158e-05\n",
      "Epoch 393/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.4074e-05\n",
      "Epoch 394/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3023e-05\n",
      "Epoch 395/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.2004e-05\n",
      "Epoch 396/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.1015e-05\n",
      "Epoch 397/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.0055e-05\n",
      "Epoch 398/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9125e-05\n",
      "Epoch 399/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8222e-05\n",
      "Epoch 400/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.7346e-05\n"
     ]
    }
   ],
   "source": [
    "# Denoising AutoEncoder example\n",
    "\n",
    "# Make an instance of a DAE\n",
    "dae = df.DAE(name=\"Simple DAE\", inputN=1)\n",
    "\n",
    "# Set inputs, inner layers and out layers\n",
    "dae.setInputs([{'shape': (10,), 'name': 'Input Layer'}])\n",
    "dae.setEncoderLayers([[{'units': 10}]])\n",
    "dae.setHiddenLayers([{'units': 5, 'activation': 'elu'}])\n",
    "dae.setDecoderLayers([{'units': 10}])\n",
    "dae.setOutLayers([{'units': 10, 'activation': 'linear'}])\n",
    "\n",
    "# # Configure the model\n",
    "dae.setModelConfiguration(optimizer='adam', loss='mse')\n",
    "\n",
    "# Build the model and print the summary\n",
    "dae.build()\n",
    "dae.summary()\n",
    "\n",
    "# Train the model\n",
    "import numpy as np\n",
    "x = np.array([[0.6, 0.4, 0.55, 0.43, 0.51, 0.61, 0.48, 0.56, 0.49, 0.51]], dtype=np.float32)\n",
    "y = np.array([[0.5, 0.5, 0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5,  0.5]], dtype=np.float32)\n",
    "\n",
    "dae.fit(x=[x], y=y, epochs=400, verbose=1)\n",
    "\n",
    "## Save the model\n",
    "# dae.save('DAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1419397]\n"
     ]
    }
   ],
   "source": [
    "y = dae.predict(x)\n",
    "print(y.numpy().T[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50ca6beb359c45098690b56dc2734b3d52d066029652770f8ebff3a62c9c1e26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
